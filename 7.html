<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Data science : modélisation, machine learning et algorithmes</title>
    <meta charset="utf-8" />
    <meta name="author" content="Samuel Goëta, Sciences Po Aix" />
    <meta name="date" content="2020-11-23" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/datactivist.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/datactivist-fonts.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data science : modélisation, machine learning et algorithmes
## Enjeux numériques de l’information
### Samuel Goëta, Sciences Po Aix
### 2020-11-23

---


layout: true

&lt;style&gt;
.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: #e95459;
}

&lt;/style&gt;


&lt;div class='my-footer'&gt;&lt;span&gt;Modélisation, machine learning et algorithmes&lt;/span&gt; &lt;center&gt;&lt;div class=logo&gt;&lt;img src='https://github.com/datactivist/slides_datactivist/raw/master/inst/rmarkdown/templates/xaringan/resources/img/fond_noir_monochrome.png' width='100px'&gt;&lt;/center&gt;&lt;/span&gt;&lt;/div&gt; 


---

class: center, middle

Ces slides en ligne : https://samgoeta.github.io/spx_eni/7.html

Sources : https://github.com/samgoeta/spx_eni/


Les productions de Datactivist sont librement réutilisables selon les termes de la licence [Creative Commons 4.0 BY-SA](https://creativecommons.org/licenses/by-sa/4.0/legalcode.fr).

&lt;BR&gt;
&lt;BR&gt;

![](https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png)


---
class:center, middle, inverse

# 1 - Data science is the new statistics?

---

### Au commencement était la statistique

.pull-left[
- une vieille science (18e siècle), pour aider les États (_Statistik_) mais aussi des entreprises privées (au départ, les assureurs =&gt; actuariat)

- fondée sur les probabilités

- faite par des mathématiciens

- forte dimension théorique
]

.pull-right[
.center[.reduite3[![](https://datactivist.coop/opendatadays/1/img/asterix1.png)]]

.footnote[*Asterix chez les pictes*, © Albert René 2013]
]

---
class:middle
### Data science : les nouvelles statistiques ?

&gt; Je continue à dire que le travail sexy dans les dix prochaines années sera celui de statisticien. Les gens pensent que je plaisante, mais qui aurait deviné que les ingénieurs en informatique auraient été le métier sexy des années 1990 ?
Hal Varian (Chief economist, Google), The McKinsey Quarterly, January 2009


&gt; “Je pense que data-scientist est un terme glamour pour désigner un statisticien”.
[Nate Silver](http://www.statisticsviews.com/details/feature/5133141/Nate-Silver-What-I-need-from-statisticians.html)


---
### Un métier au croisement de trois compétences 
.pull-left[
[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/data-science-venn-diagram.png)]]](http://www.prooffreader.com/2016/09/battle-of-data-science-venn-diagrams.html)
]

.pull-right[

La data science, comparativement à la statistique "traditionnelle", est un métier de praticien, presque de bidouilleur : elle nécessite des compétences mathématiques et statistiques, certes, mais aussi une compétence "métier" (compréhension du domaine d'application) et une solide maîtrise de l'informatique. 
]


---

### Les 4 V du Big data

![](https://www.usine-digitale.fr/mediatheque/0/3/5/000351530/big-data.jpg)


---
### Exemple de big data : l'algorithme de Spotify 

[![](img/spotify.png)](https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe)

[![](img/spoti.png)](https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe)



---

### L'obsession du volume de données

.pull-left[
**Quelques chiffres omniprésents** : 
- le volume de données produit double tous les 3 ans (Gantz &amp; Reisel 2011)
- 90% des données créées dans les deux dernières années (IBM 2012)
- 40% : croissance annuelle de la production de données (Maniyka et al. 2011) 
]

--

.pull-right[
**Problèmes de cette approche** : 
- Estimations guidées par des intérêts commerciaux
- Ne définit pas ce que sont ces données
- Résume le big data au Volume
-Explique mal la mise en données du monde

]

---

### Les promesses du big data

** Selon vous, quels sont les intérêts du big data ?**

--

Kitchin (2014) résume les promesses du big data : 
- **“Administrer les citoyens”** : dans la continuité de la statistique, améliorer la connaissance de l’administration et prédire l(par exemple, es crimes
)
- **“Gérer les organisations”** : améliorer le fonctionnement de toutes les composantes de l’organisation par l’exploitation des données

- **“Accroître la valeur”** : micro-ciblage marketing, optimisation des magasins et des opérations, efficience de la chaine

- **“Créer de meilleurs endroits”** : gouverner les villes avec des données (smart city) 

- **Un nouveau paradigme scientifique** : une nouvelle ère guidée par les corrélations
, sans théorie

---
class:inverse, center, middle

# 2 - Le rôle de l'informatique dans le développement de la data science

---
### Développement de la puissance de calcul

[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/moore.png)]]](http://visual.ly/infographic-about-computers)

---
### Développement de la capacité de stockage

1996... 
[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/altavista.png)]]](https://twitter.com/alicemazzy/status/655306196128280576?ref_src=twsrc%5Etfw)


---
### Développement de la capacité de stockage
2016...

[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/amazon.png)]]](https://aws.amazon.com/blogs/aws/aws-snowmobile-move-exabytes-of-data-to-the-cloud-in-weeks/)

---
### Développement de la capacité de stockage

2016... 

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/snowmobile.png)]]

---
### Développement de la capacité de stockage

[.reduite[.center[![](./img/atacama.png)]]](https://aws.amazon.com/blogs/aws/aws-snowmobile-move-exabytes-of-data-to-the-cloud-in-weeks/)


---
### Pour résumer

Autrefois, on travaillait sur :

- de "petits" jeux de données (aussi bien en termes de nombre de lignes que de colonnes)

- avec des valeurs numériques ou transformées en nombres

- des modèles simples, voire simplistes, pour pouvoir facilement être estimés

Aujourd'hui, on travaille avec :

- des données parfois massives

- qui peuvent porter sur des nombres, mais aussi du texte, des images, des vidéos...

- et des modèles aussi complexes qu'on veut, qui peuvent être estimés grâce à des méthodes de simulation dans un contexte de disponibilité massive de la puissance de calcul. 

---

### Modélisation

- Un modèle réduit de la réalité pour analyser, expliquer ou prédire

Modéliser, c’est mettre en relation une *variable expliquée*
(dépendante / prédite) et une ou plusieurs *variables explicatives*
(indépendantes / prédicteurs).


---
### Pourquoi modéliser ?

.reduite[.center[![](https://datactivist.coop/opendatadays/1/img/Hibbs.jpg)]]


---
### Pourquoi modéliser ? Les ["deux cultures"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)

- pour analyser et expliquer

- pour prédire

---
### Pourquoi modéliser ? Les ["deux cultures"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)

- pour analyser et expliquer (**statistiques classiques, économétrie**)

- pour prédire (**machine learning, IA...**)

### Tous les modèles sont faux, certains sont utiles


--- 
### Exemple de modèle : FiveThirtyEight

[![](img/538.png)](https://projects.fivethirtyeight.com/trump-biden-election-map/?cid=abcnews)

---
### Exercice avec FiveThirty Eight

Rendez vous sur la page de méthodologie des prédictions électorales de FiveThirtyEight : https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/

Constituez 4 groupes : 
- step 1 : sondages

- step 2 : données fondamentales

- step 3 : incertitude

- step 4 : ajustements

#### Chaque groupe devra préparer  une présentation de 5 minutes max au prochain cours 


---
## All models are wrong, some are useful

&gt; Since all models are wrong the scientist cannot obtain a "correct" one by excessive elaboration. On the contrary following William of Occam he should seek an economical description of natural phenomena. 
&gt; *Puisque tous les modèles sont faux, le scientifique ne peut pas en obtenir un "correct" en élaborant toujours davantage. Au contraire, en suivant Guillaume d'Ockham, il devrait chercher une description économe des phénomènes naturels.*
[George Box](https://dx.doi.org/10.1080%2F01621459.1976.10480949)

---
## All models are wrong, some are useful

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/ockham.jpg)]]

---
class: inverse, middle, center

# 3 - Les modèles statistiques


---
### Modèles statistiques

Du fait des deux finalités possibles (comprendre et prédire), tout modèle statistique est un compromis entre intelligibilité et fidélité aux données. Comprendre exige de se concentrer sur un petit nombre de variables, et d'accepter un degré d'erreur relativement élevé. Prédire conduit à minimiser l'erreur, et pour cela inclure un grand nombre de variables, dont certaines peuvent ne fournir aucune valeur ajoutée en termes de compréhension mais être très prédictives.

Par exemple, le meilleur prédicteur de la taille du pied droit d'un individu est la taille... de son pied gauche. Très utile pour prédire, pas du tout pour comprendre !

---
### Estimation d'un modèle

Dans le cas d'un modèle linéaire (les plus courants), on recoure pour l'estimation à la méthode des moindres carrés ordinaires (MCO, ou OLS en anglais). Elle présente l'avantage de ne pas nécessiter de puissance de calcul importante, et permettait ainsi l'estimation de modèles avant l'invention de l'ordinateur.

Cette page propose un exemple interactif pour comprendre la méthode des moindres carrés : http://setosa.io/ev/ordinary-least-squares-regression/


---
### Underfitting et overfitting

Dans un modèle il faut trouver un équilibre entre le risque d'une part de ne pas assez "coller" aux données (underfitting), et celui d'autre part de trop y coller (overfitting). Dans le premier cas on ignore de l'information, dans le second on prend pour de l'information ce qui n'est que du bruit.   

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/underfit.png)]]

---
### Underfitting et overfitting

Pour éviter l'overfitting :

- le principe fondamental est de différencier les données d'apprentissage (sur lesquelles on estime le modèle) et les données de test (sur lesquelles on évalue la qualité prédictive du modèle) 

- En outre, il ne faut utiliser les données de confirmation (test) qu'une fois : en effet une fois qu'on a sélectionné un modèle sur la base de ces données de test, ce sont devenues des données d'apprentissage...

- On peut utiliser des méthodes dites de pénalisation : elles consistent à favoriser des modèles "simples" plutôt que des modèles "complexes" 

---
### Extrapolation

Un modèle underfitté ou overfitté sera très mauvais en extrapolation (prédiction au-delà des valeurs connues du modèle). L'extrapolation est, de toute façon, difficile...

[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/sinus.png)]]](http://r4ds.had.co.nz/model-basics.html)

---
class: inverse, center, middle

# 3 - Et le machine learning alors ?

---
## Et le machine learning alors ?

- Fondamentalement, modélisation et machine learning ne sont pas différents, du point de vue d'un statisticien : il s'agit de modéliser un `\(Y\)` en fonction d'un ensemble de variables/*features* `\(X_i\)`

- une des différences principales toutefois : veut-on prévoir (machine learning) ou comprendre/analyser (modélisation statistique traditionelle) ?

- et donc : peut-on, veut-on interpréter les coefficients (modélisation statistique traditionnelle : oui, machine learning : non) ?

- en pratique, le machine learning porte généralement sur des données plus complexes (volumétrie plus importante, données de type image, vidéo, son, langage naturel...) que la modélisation traditionnelle

- Elles comportent aussi souvent beaucoup de valeurs manquantes. 

---
## Et le machine learning alors ?

En *machine learning* on travaille souvent à partir de ce genre de tableau (notes attribuées par des lecteurs à des livres - on veut pouvoir prédire les valeurs des cases vides) :

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/ratings.png)]]

---
## Concepts de machine learning

- Apprentissage **supervisé** (on donne à la machine des données déjà classifiées) vs **non supervisé** (on laisse la machine dégager ses propres classifications)

- Apprentissage supervisé : il faut des données déjà classées/étalonnées. Souvent à la main ! =&gt; [*#digitallabour*](https://fr.wikipedia.org/wiki/Travail_num%C3%A9rique)

![](https://datactivist.coop/SPoSGL/sections/img/turk.jpg)

---
## Apprentissage supervisé 

Par exemple, les *reCaptcha* permettent à Google d'améliorer ses algorithmes d'OCR (pour Google Books ou Google Street View, notamment). 

[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/captcha.jpg)]]](https://fakecaptcha.com)

---
## Apprentissage supervisé

On peut par exemple détecter automatiquement l'orientation d'un toit, pour déterminer son potentiel de production d'électricité photovoltaïque. 

[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/opensolarmap.png)]]](https://opensolarmap.org)

---
## Apprentissage supervisé

Ou encore prédire si un email est prioritaire ou non. 

[.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/inbox.png)]]](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36955.pdf)

---
## Apprentissage non supervisé 


- Le problème majeur auquel répond l'apprentissage non supervisé est celui de la réduction de la dimensionnalité : on a des données caractérisées par des dizaines/centaines/milliers/millions de variables, et donc difficiles à explorer. 

- Ces jeux de données à très haute dimensionnalité sont impossibles à explorer visuellement. Comment simplifier l'information et la résumer ?

---
## Apprentissage non supervisé 

Exemple de l'ensemble des votes au Congrès américain de 1879 à 2015 : des milliers de variables, résumées en une seule dimension ("liberal" vs "conservative"). 

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/house.png)]]

---
## Apprentissage non supervisé 

Il est notamment beaucoup utilisé sur de vastes corpus textuels, pour identifier des sujets par exemple.

L'exemple d'un algorithme largement utilisé (LDA) de détection de sujets dans un vaste corpus textuel : https://gallery.shinyapps.io/LDAelife/


---
## Quelques algorithmes notoires

- la régression (linéaire, logistique, LOESS...), éventuellement régularisée (Ridge, LASSO...)

- les arbres de décision

- Naive Bayes (indépendance des attributs)

- Réseau bayésien (graphe de corrélation entre variables, inclut des _priors_)

- réseau de neurones (perceptron, back-propagation, Hopfield network...)

- Deep learning

- Random forest, Boosting, Gradient Boosting

- Support vector machine

- etc. 

---
## Apprentissage non supervisé

- clustering : k-means, k-medians, CAH... 

- réduction de la dimensionnalité : ACP, analyse géométrique des données, MDS, LDA topic clustering...

- analyses de réseau

- word2vec

- etc.

---
## Apprentissage non supervisé

Le développement des réseaux de neurone permet le développement assez convaincant de méthodes non supervisées... à condition d'avoir des corpus suffisamment importants ! Voir l'exemple des word embeddings, qui permet de restituer de manière automatique de la sémantique. 

[Word2vec](https://www.tensorflow.org/tutorials/word2vec) :

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/word2vec.png)]]


---
## Le cas du deep learning

Le *deep learning* est devenu l'une des méthodes d'apprentissage les plus en vogue. Il s'appuie sur la méthode des "réseaux de neurone", remise au goût du jour par le Français Yann Le Cun (aujourd'hui directeur de la recherche en IA chez Facebook, prix Turing 2019) dans les années 1990. 

Comment apprendre à un ordinateur à lire ? Ce qu'un ordinateur "voit", ce ne sont pas des chiffres, mais des pixels en niveaux de gris. 

![largeur](https://datactivist.coop/SPoSGL/sections/img/digits.png)

---
## Le cas du deep learning

Prenons une centaines de chiffres manuscrits. Comment permettre à l'ordinateur de deviner de quels chiffres il s'agit ? 

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/mnist_100_digits.png)]]

---
## Le cas du deep learning

Le concept de base des réseaux de neurones est celui du perceptron. D'un point de vue logique, il s'agit d'une cellule (comparable à un neurone) transformant plusieurs inputs en un output.   

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/perceptron.png)]]

---
## Le cas du deep learning

On parle de "réseaux de neurones" car on couple plusieurs de ces perceptions, sur plusieurs couches. En articulant ainsi de nombreuses cellules réalisant des opérations simples, on peut au final réaliser des opérations très complexes, par exemple reconnaître un chiffre. 

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/smallnetwork.png)]]


---
## Le cas du deep learning

On parle de "couches cachées" pour évoquer les couches du réseaux de neurones situées entre celle des inputs et celle des outpits. On parle d'apprentissage pprofond du fait de l'empilement de ces couches cachées.  

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/layers.png)]]

---
## Le cas du deep learning

La valeur des inputs est celle prise par chaque pixel (il y en a 256) de l'image. 

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/number.gif)]]

---
## Le cas du deep learning

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/hiddenlayer.png)]]

---
## Le cas du deep learning

L'estimation des poids (~ coefficients) associés à chaque neurone se fait par une méthode dite du "gradient descent", qui repose sur l'utilisation de la simulation et donc de moyens de calculs très puissants. 

.reduite[.center[![](https://datactivist.coop/SPoSGL/sections/img/valley_with_ball.png)]]



---
## Le cas du deep learning

Voici un (petit) réseau de neurones en action. 

http://playground.tensorflow.org/

---
## Le cas du deep learning

Le deep learning permet de décomposer un problème complexe (reconnaissance faciale, par exemple) en un grand nombre de problèmes simples.  

=&gt; beaucoup de couches cachées 

=&gt; permet l'émergence de la notion d'**abstraction** (un visage se décompose en un nez, une bouche, des yeux, des oreilles...)


---
## Les points auxquels prêter attention

- *feature selection* : quels sont les éléments qu'on donne à l'algorithme d'apprentissage, et comment sont-ils qualifiés ?

- régularisation (favorise les modèles simples par rapport aux modèles compliqués, pour éviter l'*overfitting*)

- qualité du critère d'optimisation

- qualité des données d'input (*Garbage in, garbage out !*)

- ...

---

class: inverse, center, middle

# Ressource complémentaire

## [Comment reconnaître un hot dog ?](https://medium.com/@jaimejcheng/hot-dog-or-not-a-lesson-on-image-classification-using-convolutional-neural-networks-a2b92e81f4ad) 

---

---
class: inverse, center, middle

## 3 - Les étapes de l'analyse des données

### Le data pipeline

---

Formalisé par la [School of data](https://schoolofdata.org/methodology/), il vise à modéliser les différentes étapes d'un projet d'analyse de données.

.reduite[.center[![](./img/datapipeline.png)]]

---
### Le data pipeline


Il a également été formalisé, de manière légèrement différente, par Hadley Wickham (Chief Scientist Officer chez Rstudio) dans un contexte de data 

Il est intéressant de noter que cette version met en évidence la dimension itérative : on essaie, on corrige, on recommence...jusqu'à ce que le résultat soit stabilisé et donc communicable.

[.reduite[.center[![](/img/data-science.png)]]](https://r4ds.had.co.nz/introduction.html)

---
### Définir les données dont on a besoin

.center[![](https://datactivist.coop/SPoSGL/sections/img/define.png)]

Cette étape est essentielle. Il s'agit de traduire une problématique concrète, en identifiant quelles données permettraient de la résoudre. Idéalement, c'est elle qui détermine les données mobilisées... mais parfois, on n'a pas le choix et on doit être opportuniste. 



---
### Trouver les données

.center[![](https://datactivist.coop/SPoSGL/sections/img/find.png)]

Une fois les données qu'on recherche identifiées, encore faut-il effectivement les trouver !
À l'avenir, assistera-t-on au développement d'un nouveau métier de "conciergerie de données" ?

Où chercher ?
- portails open data
- dépôts divers (internes aux organisations ou publics)
- data brokers
- s'adresser au chief data officer
- etc.

---
### Acquérir les données

.center[![](https://datactivist.coop/SPoSGL/sections/img/get.png)]

Il s'agit d'importer les données dans son outil d'analyse (Excel, logiciel spécialisé, langage de programmation...).

Des outils dédiés, souvent qualifiés d'ETL (extract / transform / load), existent (ex : Talend).

???

Connecteurs
ETL : ex Talend, Logstash


---
### Vérifier les données

.center[![](https://datactivist.coop/SPoSGL/sections/img/verify.png)]

La qualité des données est-elle correcte ? Les données sont-elles à jour ? Bien documentées ? Exhaustives ? 

Il est important de pratiquer un "sanity check" : vérifier sur un échantillon de données qu'elles n'ont pas l'air aberrantes par rapport à ce qu'on sait déjà.

???

Importance d'un sanity check

---
### Vérifier les données

.center[[![](https://datactivist.coop/SPoSGL/sections/img/sanity.jpg)](http://www.erogol.com/ml-work-flow-part-4-sanity-checks-data-spliting/)]



---
### Nettoyer les données

.center[![](https://datactivist.coop/SPoSGL/sections/img/clean.png)]

Les données sont rarement dans la forme dont on a besoin pour pouvoir les analyser... Il faut donc les nettoyer, les mettre en forme. 

On peut pour se faire par exemple s'appuyer sur le paradigme du [*tidy data*](https://r4ds.had.co.nz/tidy-data.html) (données "bien rangées") : une ligne par observation, une colonne par variable, une valeur par case.


---
### Analyser les données

.center[![](https://datactivist.coop/SPoSGL/sections/img/analyse.png)]

C'est la partie à laquelle on pense spontanément quand on parle de *data science*, qui fait fantasmer les scénaristes et les professionnels du marketing... mais qui représente environ 20 % du temps consacré à un projet de *data science*. 

C'est à cette phase que se fait la modélisation, sur laquelle nous revenons dans la dernière section. 




---
## Communiquer les résultats

.center[![](https://datactivist.coop/SPoSGL/sections/img/present.png)]

L'analyse une fois stabilisée, il faut la communiquer à son audience (public, chercheurs, décideur...). De multiples formes sont possibles, on parlera parfois de *data science product* :

- rapport
- [recherche reproductible](https://r4ds.had.co.nz/r-markdown.html)
- datavisualisation
- dashboard
- [application interactive](http://shiny.rstudio.com/)
- etc.


---
class: inverse, center, middle

# Merci !

Contact : [samuel@datactivist.coop](mailto:joel@datactivist.coop)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">\n  <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">\n  </div>\n</div>`\n"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
